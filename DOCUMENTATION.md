## üìã Y√™u C·∫ßu H·ªá Th·ªëng

- Docker Desktop 4.0+
- Docker Compose 2.0+
- Minimum 8GB RAM (16GB recommended)
- 20GB free disk space
- Windows 10/11, macOS, ho·∫∑c Linux

## üöÄ C√†i ƒê·∫∑t v√† Ch·∫°y

### 1. Chu·∫©n B·ªã

```bash
# Clone ho·∫∑c navigate ƒë·∫øn th∆∞ m·ª•c d·ª± √°n
cd bigdata-cloud
```

### 2. C·∫•u H√¨nh Environment

```bash
# Copy file c·∫•u h√¨nh m·∫´u
copy .env.example .env

# Ch·ªânh s·ª≠a .env n·∫øu c·∫ßn (optional)
notepad .env
```

### 3. Kh·ªüi ƒê·ªông H·ªá Th·ªëng

**Windows:**
```bash
scripts\start.bat
```

**Linux/Mac:**
```bash
chmod +x scripts/start.sh
./scripts/start.sh
```

Script s·∫Ω t·ª± ƒë·ªông:
- ‚úÖ Build Docker images cho t·∫•t c·∫£ services
- ‚úÖ Kh·ªüi ƒë·ªông Kafka, Zookeeper, Redis, MongoDB
- ‚úÖ T·∫°o Kafka topics (user-events, user-interactions, video-metadata)
- ‚úÖ Initialize MongoDB v·ªõi indexes
- ‚úÖ Start Spark cluster (master + 2 workers)
- ‚úÖ Start API service v√† Data Generator

### 4. Ch·∫°y Spark Streaming Job

M·ªü terminal m·ªõi v√† ch·∫°y:

```bash
python scripts/submit_spark_job.py
```

Job n√†y s·∫Ω:
- Consume events t·ª´ Kafka topics
- Process real-time user interactions
- Update user profiles trong Redis v√† MongoDB
- Calculate engagement metrics

### 5. Training Recommendation Model

```bash
python scripts/train_model.py
```

Training process:
- Build user-item interaction matrix
- Train ALS model v·ªõi implicit feedback
- Calculate item-item similarity matrix
- Update trending videos
- Save model to disk

### 6. Ki·ªÉm Tra H·ªá Th·ªëng

Truy c·∫≠p c√°c endpoint sau:

- **Web Dashboard**: http://localhost:5000/static/index.html
- **API Health Check**: http://localhost:5000/health
- **Spark UI**: http://localhost:8080
- **Get Recommendations**: http://localhost:5000/api/v1/recommendations/user_1

## üìä Truy C·∫≠p H·ªá Th·ªëng

| Service | URL/Host | Credentials |
|---------|----------|-------------|
| Web Dashboard | http://localhost:5000/static/index.html | - |
| REST API | http://localhost:5000 | - |
| Spark UI | http://localhost:8080 | - |
| Kafka | localhost:9092 | - |
| Redis | localhost:6379 | - |
| MongoDB | localhost:27017 | admin/admin123 |

## üîå API Endpoints

### 1. Health Check
```http
GET /health

Response:
{
  "status": "healthy",
  "timestamp": "2024-12-21T10:00:00",
  "services": {
    "kafka": "connected",
    "redis": "connected",
    "mongodb": "connected"
  }
}
```

### 2. Get Recommendations
```http
GET /api/v1/recommendations/{user_id}?n=20&method=als

Parameters:
- user_id: ID c·ªßa user (required)
- n: S·ªë l∆∞·ª£ng recommendations (default: 20)
- method: 'als' ho·∫∑c 'item_cf' (default: 'als')

Response:
{
  "user_id": "user_1",
  "recommendations": [
    {
      "video_id": "video_123",
      "score": 0.85,
      "method": "als",
      "title": "Video Title",
      "category": "comedy",
      "duration": 45.5
    },
    ...
  ],
  "count": 20
}
```

### 3. Track User Event
```http
POST /api/v1/events/track
Content-Type: application/json

Body:
{
  "event_type": "view",
  "user_id": "user_123",
  "video_id": "video_456",
  "watch_time": 30.5,
  "total_duration": 60.0
}

Response:
{
  "status": "success",
  "message": "Event tracked successfully"
}
```

### 4. Get Trending Videos
```http
GET /api/v1/trending?n=20

Response:
{
  "trending": [
    {
      "video_id": "video_789",
      "score": 1250.0,
      "method": "trending",
      "title": "Trending Video"
    },
    ...
  ],
  "count": 20
}
```

### 5. Get User Profile
```http
GET /api/v1/user/{user_id}/profile

Response:
{
  "profile": {
    "user_id": "user_1",
    "total_interactions": 150,
    "favorite_categories": ["comedy", "music"],
    "recent_videos": [...],
    "last_activity": "2024-12-21T10:00:00"
  },
  "recent_interactions": [...]
}
```

### 6. Get System Statistics
```http
GET /api/v1/stats

Response:
{
  "total_users": 10000,
  "total_videos": 50000,
  "total_interactions": 500000,
  "active_sessions": 1500,
  "timestamp": "2024-12-21T10:00:00"
}
```

## üìÅ C·∫•u Tr√∫c Th∆∞ M·ª•c

```
bigdata-cloud/
‚îú‚îÄ‚îÄ docker-compose.yml          # Docker orchestration
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ .env.example               # Environment variables template
‚îÇ
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ api/                   # Flask REST API
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.py            # Main API application
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ static/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ index.html    # Web dashboard
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ data-generator/        # Event simulator
‚îÇ       ‚îú‚îÄ‚îÄ event_generator.py # Generates fake user events
‚îÇ       ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ       ‚îî‚îÄ‚îÄ requirements.txt
‚îÇ
‚îú‚îÄ‚îÄ spark-jobs/
‚îÇ   ‚îú‚îÄ‚îÄ streaming_consumer.py  # Spark Streaming job
‚îÇ   ‚îî‚îÄ‚îÄ recommendation_engine.py  # ML recommendation models
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ data_models.py         # Data class definitions
‚îÇ   ‚îî‚îÄ‚îÄ storage.py             # MongoDB & Redis operations
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ start.sh               # Startup script (Linux/Mac)
‚îÇ   ‚îú‚îÄ‚îÄ start.bat              # Startup script (Windows)
‚îÇ   ‚îú‚îÄ‚îÄ submit_spark_job.py    # Submit Spark streaming job
‚îÇ   ‚îú‚îÄ‚îÄ train_model.py         # Train recommendation model
‚îÇ   ‚îî‚îÄ‚îÄ init-mongo.js          # MongoDB initialization
‚îÇ
‚îî‚îÄ‚îÄ data/
    ‚îú‚îÄ‚îÄ models/                # Trained ML models
    ‚îî‚îÄ‚îÄ checkpoints/           # Spark checkpoints
```

## üî¨ Thu·∫≠t To√°n Recommendation

### 1. Collaborative Filtering (ALS)
- **Matrix Factorization** s·ª≠ d·ª•ng Alternating Least Squares
- **Implicit Feedback** t·ª´ watch time, likes, shares, comments
- **Hyperparameters**: 
  - Factors: 64
  - Iterations: 15
  - Regularization: 0.01

### 2. Item-based Collaborative Filtering
- **Cosine Similarity** gi·ªØa videos d·ª±a tr√™n user interaction patterns
- **Similarity Threshold**: 0.3
- Real-time updates t·ª´ Redis cache

### 3. Hybrid Approach
- K·∫øt h·ª£p ALS v√† item-based CF
- Weighted scoring based on event types
- Real-time preference updates

### 4. Event Weighting
```python
View with completion_rate > 0.8: weight = 5.0
View with completion_rate > 0.5: weight = 3.0
Like: weight = 10.0
Share: weight = 15.0
Comment: weight = 8.0
Skip: weight = -2.0
```

### 5. Cold Start Strategy
- **New Users**: Trending videos trong 24h
- **New Videos**: Category-based recommendations
- **Fallback**: Popular videos by engagement score

## üìà Monitoring & Debugging

### View Service Logs
```bash
# T·∫•t c·∫£ services
docker-compose logs -f

# Specific service
docker-compose logs -f api-service
docker-compose logs -f spark-master
docker-compose logs -f kafka
docker-compose logs -f data-generator
```

### Kafka Management
```bash
# List topics
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092

# Describe topic
docker exec kafka kafka-topics --describe --topic user-events --bootstrap-server localhost:9092

# Consumer groups
docker exec kafka kafka-consumer-groups --bootstrap-server localhost:9092 --list

# View messages (debug)
docker exec kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic user-events --from-beginning --max-messages 10
```

### MongoDB Inspection
```bash
# Connect to MongoDB
docker exec -it mongodb mongosh -u admin -p admin123 --authenticationDatabase admin

# Common queries
use video_recommendation
db.interactions.countDocuments()
db.users.find().limit(5).pretty()
db.videos.find().sort({engagement_score: -1}).limit(10)
db.user_profiles.findOne({user_id: "user_1"})
```

### Redis Inspection
```bash
# Connect to Redis
docker exec -it redis redis-cli

# Common commands
KEYS user:*
ZRANGE trending:videos:24h 0 10 WITHSCORES
HGETALL user:user_1:stats
GET recommendations:user_1
```

### Spark Job Status
```bash
# Check Spark master logs
docker logs spark-master

# Check worker logs
docker logs spark-worker-1
docker logs spark-worker-2

# Access Spark UI
# Navigate to http://localhost:8080
```

## üß™ Testing

### Manual API Testing
```bash
# Health check
curl http://localhost:5000/health

# Get recommendations
curl "http://localhost:5000/api/v1/recommendations/user_1?n=10&method=als"

# Track event
curl -X POST http://localhost:5000/api/v1/events/track \
  -H "Content-Type: application/json" \
  -d "{\"event_type\":\"like\",\"user_id\":\"user_1\",\"video_id\":\"video_100\"}"

# Get trending
curl http://localhost:5000/api/v1/trending?n=20

# Get stats
curl http://localhost:5000/api/v1/stats
```

### Load Testing (Optional)
```bash
# Install Apache Bench
# Windows: Download from Apache website
# Linux: sudo apt-get install apache2-utils

# Run load test
ab -n 1000 -c 10 http://localhost:5000/api/v1/recommendations/user_1
```

## üêõ Troubleshooting

### Problem: Kafka kh√¥ng start
```bash
# Solution: X√≥a data v√† restart
docker-compose down -v
docker-compose up -d zookeeper
# Wait 10 seconds
docker-compose up -d kafka
```

### Problem: Spark job fails v·ªõi OutOfMemory
```bash
# Solution: TƒÉng memory allocation
# Edit docker-compose.yml:
environment:
  - SPARK_WORKER_MEMORY=4G
  - SPARK_DRIVER_MEMORY=4G

# Ho·∫∑c gi·∫£m s·ªë workers
docker-compose up -d spark-master spark-worker-1
```

### Problem: MongoDB connection refused
```bash
# Solution: Check MongoDB status
docker-compose logs mongodb

# Restart MongoDB
docker-compose restart mongodb

# Check connection
docker exec -it mongodb mongosh -u admin -p admin123 --authenticationDatabase admin
```

### Problem: Redis connection timeout
```bash
# Solution: Restart Redis
docker-compose restart redis

# Check Redis
docker exec -it redis redis-cli PING
```

### Problem: API returns 500 error
```bash
# Check API logs
docker-compose logs api-service

# Restart API
docker-compose restart api-service

# Verify all dependencies are running
docker-compose ps
```

## üîÑ Stopping and Cleanup

### Stop All Services
```bash
docker-compose down
```

### Stop and Remove All Data
```bash
docker-compose down -v
```

### Stop Specific Service
```bash
docker-compose stop api-service
```

### Restart Service
```bash
docker-compose restart api-service
```

### Rebuild After Code Changes
```bash
docker-compose build api-service
docker-compose up -d api-service
```

## ‚öôÔ∏è Advanced Configuration

### TƒÉng Kafka Throughput
```bash
# TƒÉng s·ªë partitions
docker exec kafka kafka-topics --alter --topic user-events \
  --partitions 6 --bootstrap-server localhost:9092
```

### Spark Tuning
Edit [spark-jobs/streaming_consumer.py](spark-jobs/streaming_consumer.py):
```python
spark = SparkSession.builder \
    .config("spark.executor.memory", "4g") \
    .config("spark.executor.cores", "4") \
    .config("spark.sql.shuffle.partitions", "200") \
    .getOrCreate()
```

### Redis Memory Management
```bash
docker exec redis redis-cli CONFIG SET maxmemory 2gb
docker exec redis redis-cli CONFIG SET maxmemory-policy allkeys-lru
```

## üìö T√†i Li·ªáu Tham Kh·∫£o

- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
- [Apache Spark Streaming Guide](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
- [Implicit Library (ALS)](https://implicit.readthedocs.io/)
- [MongoDB Best Practices](https://www.mongodb.com/docs/manual/administration/production-notes/)
- [Redis Commands Reference](https://redis.io/commands/)
- [Flask Documentation](https://flask.palletsprojects.com/)

## ü§ù Contributing

Contributions are welcome! Vui l√≤ng t·∫°o Pull Request ho·∫∑c Issue tr√™n GitHub.

## üìÑ License

This project is licensed under the MIT License.

## üë• Authors

Project developed for Big Data & Cloud Computing course.

---

